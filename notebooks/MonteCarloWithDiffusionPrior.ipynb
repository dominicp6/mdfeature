{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad73c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import torch\n",
    "import autograd.numpy as np\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def hamiltonian_monte_carlo(n_samples, negative_log_prob, initial_position, C, Q, gamma, path_len=1, step_size=0.5):\n",
    "    \"\"\"Run Hamiltonian Monte Carlo sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to return\n",
    "    negative_log_prob : callable\n",
    "        The negative log probability to sample from\n",
    "    initial_position : np.array\n",
    "        A place to start sampling from.\n",
    "    path_len : float\n",
    "        How long each integration path is. Smaller is faster and more correlated.\n",
    "    step_size : float\n",
    "        How long each integration step is. Smaller is slower and more accurate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Array of length `n_samples`.\n",
    "    \"\"\"\n",
    "    # autograd magic\n",
    "    #dVdq = grad(negative_log_prob)\n",
    "    initial_position = torch.flatten(initial_position)\n",
    "\n",
    "    # collect all our samples in a list\n",
    "    samples = [initial_position]\n",
    "\n",
    "    # Keep a single object for momentum resampling\n",
    "    momentum = st.norm(0, 1)\n",
    "\n",
    "    # If initial_position is a 10d vector and n_samples is 100, we want\n",
    "    # 100 x 10 momentum draws. We can do this in one call to momentum.rvs, and\n",
    "    # iterate over rows\n",
    "    size = (n_samples,torch.numel(initial_position)) \n",
    "    for p0 in momentum.rvs(size=size):\n",
    "        # Integrate over our path to get a new position and momentum\n",
    "        q_new, p_new = leapfrog(\n",
    "            samples[-1],\n",
    "            p0,\n",
    "            negative_log_prob,\n",
    "            C,\n",
    "            Q,\n",
    "            gamma,\n",
    "            path_len=path_len,\n",
    "            step_size=step_size,\n",
    "        )\n",
    "\n",
    "        # Check Metropolis acceptance criterion\n",
    "        start_log_p = negative_log_prob(torch.reshape(samples[-1],(4,4)), C, Q, gamma) - np.sum(momentum.logpdf(p0))\n",
    "        new_log_p = negative_log_prob(torch.reshape(q_new,(4,4)), C, Q, gamma) - np.sum(momentum.logpdf(p_new))\n",
    "        if np.log(np.random.rand()) < start_log_p - new_log_p:\n",
    "            samples.append(q_new)\n",
    "        else:\n",
    "            samples.append((samples[-1]).clone())\n",
    "\n",
    "    return samples[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(q, p, negative_log_prob, C, Q, gamma, path_len, step_size):\n",
    "    \"\"\"Leapfrog integrator for Hamiltonian Monte Carlo.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.floatX\n",
    "        Initial position\n",
    "    p : np.floatX\n",
    "        Initial momentum\n",
    "    dVdq : callable\n",
    "        Gradient of the velocity\n",
    "    path_len : float\n",
    "        How long to integrate for\n",
    "    step_size : float\n",
    "        How long each integration step should be\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q, p : np.floatX, np.floatX\n",
    "        New position and momentum\n",
    "    \"\"\"\n",
    "    q, p = torch.tensor(q, requires_grad=True), torch.tensor(p)\n",
    "    NLL =  negative_log_prob(torch.reshape(q, (4,4)), C, Q, gamma) #todo: fix\n",
    "    NLL.backward()\n",
    "    dVdq = torch.flatten(q.grad)\n",
    "    \n",
    "    print(\"initial q\", q)\n",
    "    \n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "    \n",
    "    q.grad.data.zero_()\n",
    "    \n",
    "    for _ in range(int(path_len / step_size) - 1):\n",
    "        \n",
    "        NLL =  negative_log_prob(torch.reshape(q, (4,4)), C, Q, gamma)\n",
    "        NLL.backward()\n",
    "        dVdq = torch.flatten(q.grad)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q += step_size * p  # whole step\n",
    "        p -= step_size * dVdq  # whole step\n",
    "        \n",
    "        q.grad.data.zero_()\n",
    "        \n",
    "    NLL =  negative_log_prob(torch.reshape(q, (4,4)), C, Q, gamma)\n",
    "    NLL.backward()\n",
    "    dVdq = torch.flatten(q.grad)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q += step_size * p  # whole step\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "    \n",
    "    q.grad.data.zero_()\n",
    "    \n",
    "    print(\"final q\", q)\n",
    "\n",
    "    # momentum flip at end\n",
    "    return q, -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 1: let's do this without inforcing reversibility         \n",
    "def compute_D(Q, x):\n",
    "    D = torch.zeros(len(Q)-1)\n",
    "    deltaQ2s = []\n",
    "    for i in range(len(Q)-1):\n",
    "        deltaQ2 = (Q[i+1]-Q[i])**2\n",
    "        D[i] = deltaQ2 * (x[i][i+1]*x[i+1][i])**(0.5)\n",
    "        deltaQ2s.append(deltaQ2)\n",
    "        \n",
    "    return D, np.mean(deltaQ2s)\n",
    "\n",
    "def compute_smoothness_correction(D, gamma, Qvar):\n",
    "    correction = 0\n",
    "    for i in range(len(D)-1):\n",
    "        correction += (D[i]-D[i+1])**2 / (2*(Qvar*gamma)**2)\n",
    "        \n",
    "    return correction\n",
    "\n",
    "def negative_log_prob(x, C, Q, gamma):\n",
    "    D, Qvar = compute_D(Q,x)\n",
    "    #print('diffusion coefficients', D)\n",
    "    #print('Qvariance', Qvar)\n",
    "    smoothness_correction = compute_smoothness_correction(D, gamma, Qvar)\n",
    "    print('smoothness correction', smoothness_correction)\n",
    "    \n",
    "    return - (C * torch.log(x)).sum() - smoothness_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.tensor([0., 0.1, 0.2, 0.3])\n",
    "C = torch.tensor([[1000., 50., 20., 10.],[48., 50., 1., 1.],[1., 1., 600., 40.],[9., 7., 33., 300.]])\n",
    "X0 = ((C + C.T)/(2*C.sum())).clone().detach().requires_grad_(True)\n",
    "print(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89744f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.numel(X0))\n",
    "samples = hamiltonian_monte_carlo(10, negative_log_prob, X0, C, Q, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLL = negative_log_prob(X0, C, Q, 0.01)\n",
    "print(NLL)\n",
    "NLL.backward()\n",
    "gradient = X0.grad\n",
    "print(gradient)\n",
    "X0.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd4198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "\n",
    "mu1 = np.ones(n) * (1.0 / 2)\n",
    "mu2 = -mu1\n",
    "\n",
    "stdev = 0.1\n",
    "sigma = np.power(stdev, 2) * np.eye(n)\n",
    "isigma = np.linalg.inv(sigma)\n",
    "dsigma = np.linalg.det(sigma)\n",
    "\n",
    "w1 = 0.1  # one mode with 0.1 of the mass\n",
    "w2 = 1 - w1  # the other mode with 0.9 of the mass\n",
    "\n",
    "\n",
    "def two_gaussians(x):\n",
    "    log_like1 = (\n",
    "        -0.5 * n * tt.log(2 * np.pi)\n",
    "        - 0.5 * tt.log(dsigma)\n",
    "        - 0.5 * (x - mu1).T.dot(isigma).dot(x - mu1)\n",
    "    )\n",
    "    log_like2 = (\n",
    "        -0.5 * n * tt.log(2 * np.pi)\n",
    "        - 0.5 * tt.log(dsigma)\n",
    "        - 0.5 * (x - mu2).T.dot(isigma).dot(x - mu2)\n",
    "    )\n",
    "    return pm.math.logsumexp([tt.log(w1) + log_like1, tt.log(w2) + log_like2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393adcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    X = pm.Uniform(\n",
    "        \"X\",\n",
    "        shape=n,\n",
    "        lower=-2.0 * np.ones_like(mu1),\n",
    "        upper=2.0 * np.ones_like(mu1),\n",
    "        testval=-1.0 * np.ones_like(mu1),\n",
    "    )\n",
    "    llk = pm.Potential(\"llk\", two_gaussians(X))\n",
    "    trace = pm.sample_smc(2000, parallel=True)\n",
    "    az_trace = az.from_pymc3(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b400a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_trace(az_trace, compact=True, kind=\"rank_vlines\")\n",
    "ax[0, 0].axvline(-0.5, 0, 0.9, color=\"k\")\n",
    "\n",
    "ax[0, 0].axvline(0.5, 0, 0.1, color=\"k\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint\n",
    "\n",
    "C = np.array([[1000., 50., 20., 10.],[48., 50., 1., 1.],[1., 1., 600., 40.],[9., 7., 33., 300.]])\n",
    "\n",
    "def random_initialiser(C):\n",
    "    dim = int(C.shape[0] * (C.shape[0] + 1) / 2)\n",
    "    x0 = np.random.uniform(size=dim)\n",
    "    \n",
    "    return x0\n",
    "\n",
    "def extract_symmetric_components(matrix):\n",
    "    dim = matrix.shape[0]\n",
    "    symmetric_components = []\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if j >= i:\n",
    "                symmetric_components.append(matrix[i][j])\n",
    "                \n",
    "    return np.array(symmetric_components)\n",
    "\n",
    "\n",
    "def compute_X(symmetric_components):\n",
    "    dim = int((-1+np.sqrt(1+8*symmetric_components.size))/2)\n",
    "    X = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if j >= i:\n",
    "                X[i][j] = symmetric_components[int(i*dim - i*(i-1)/2 + j - i)]\n",
    "                X[j][i] = X[i][j]  # symmetrise\n",
    "                \n",
    "    return X\n",
    "\n",
    "def compute_D(Q, X, x):\n",
    "    D = np.zeros(len(Q) - 1)\n",
    "    deltaQ2s = []\n",
    "    for i in range(len(Q) - 1):\n",
    "        deltaQ2 = (Q[i + 1] - Q[i]) ** 2\n",
    "        D[i] = deltaQ2 * (X[i][i + 1]*X[i+1][i] / x[i]*x[i+1]) ** (0.5)\n",
    "        deltaQ2s.append(deltaQ2)\n",
    "\n",
    "    return D, np.mean(deltaQ2s)\n",
    "\n",
    "\n",
    "def compute_smoothness_correction(D, gamma, Qvar):\n",
    "    correction = 0\n",
    "    for i in range(len(D) - 1):\n",
    "        correction += (D[i] - D[i + 1]) ** 2 / (2 * (Qvar * gamma) ** 2)\n",
    "\n",
    "    return correction\n",
    "\n",
    "def negative_log_likelihood(X, C, Q, gamma, t, use_smoothing):\n",
    "    X = compute_X(symmetric_components = X)\n",
    "    x = np.sum(X, axis=1) #col sum\n",
    "\n",
    "    if use_smoothing:\n",
    "        D, Qvar = compute_D(Q, X, x)\n",
    "        smoothness_correction = compute_smoothness_correction(D, gamma, Qvar)\n",
    "    else:\n",
    "        smoothness_correction = 0\n",
    "        \n",
    "    A = X/x\n",
    "    \n",
    "    positive_row_barrier_function = (1/t)*(np.sum(- np.log(x))+np.sum(- np.log(1-x)))\n",
    "    \n",
    "    nll = float(- np.sum(C * np.log(A)) + smoothness_correction + positive_row_barrier_function)\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacd91b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Q = np.array([0., 0.1, 0.2, 0.3])\n",
    "C = np.array([[1000., 50., 20., 10.],[48., 50., 1., 1.],[1., 1., 600., 40.],[9., 7., 33., 300.]])\n",
    "gamma = 0.001\n",
    "use_smoothing=True\n",
    "t = 1\n",
    "X0 = np.random.uniform(0.8, 1.2, size=(C.shape))*((C + C.T)/(2*np.sum(C)))\n",
    "\n",
    "\n",
    "number_of_parameters = int(C.shape[0]*(C.shape[0]+1)/2)\n",
    "A = np.eye(number_of_parameters)\n",
    "b = np.zeros(number_of_parameters)\n",
    "lincon = LinearConstraint(A, b, np.inf*np.ones(number_of_parameters), keep_feasible=True)\n",
    "    \n",
    "x0 = extract_symmetric_components(X0)\n",
    "print(np.sum(X0, axis=1))\n",
    "print(compute_X(x0)/np.sum(compute_X(x0), axis=1))\n",
    "\n",
    "for gamma in [0.1,0.01,0.001]:\n",
    "    res = minimize(method='Nelder-Mead', fun=negative_log_likelihood, x0 = x0, args=(C, Q, gamma, t, use_smoothing), constraints=(lincon,), options={'maxiter': 20000})\n",
    "    #print(res)\n",
    "    X_star = compute_X(res.x)\n",
    "    x_star = np.sum(X_star, axis=1)\n",
    "    pi_star = x_star/np.sum(x_star)\n",
    "    #print(pi_star)\n",
    "    P_star = X_star/x_star\n",
    "    plt.imshow(P_star)\n",
    "    plt.colorbar()\n",
    "    #print(\"gamma\", gamma)\n",
    "    #print(P_star)\n",
    "    #print(x_star)\n",
    "    #print(np.sum(x_star))\n",
    "    #print(X_star)\n",
    "    #print(x_star)\n",
    "    print(P_star)\n",
    "    for i in range(X.shape[0]-1):\n",
    "        print(P_star[i][i + 1]*P_star[i+1][i])\n",
    "    D, Qvar = compute_D(Q, X_star, x_star)\n",
    "    plt.show()\n",
    "    plt.plot(D)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5d3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4722b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
