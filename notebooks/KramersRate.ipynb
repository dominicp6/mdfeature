{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kramer's Rate Evaluation of Quality of Collective Variables\n",
    "\n",
    "Start this notebook with this command:\n",
    "`env PYTHONPATH=$PATH_TO_MDFEATURE/mdfeature/src jupyter notebook`\n",
    "\n",
    "e.g. `env PYTHONPATH=/home/dominic/PycharmProjects/mdfeature/src jupyter notebook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import metadynamics\n",
    "from openmm import *\n",
    "from openmm.app import *\n",
    "from openmm.unit import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as image\n",
    "from pyemma import msm\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.pyplot import cm\n",
    "import mdtraj as md\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import numpy as np\n",
    "from autoimpute.imputations import MiceImputer, MultipleImputer, SingleImputer\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import mdfeature.diffusionmap as diffusionmap\n",
    "\n",
    "import mdfeature.fixed_point_iteration as fpi\n",
    "\n",
    "from mdfeature.KramersRateEvaluator import KramersRateEvaluator\n",
    "\n",
    "import dill\n",
    "\n",
    "from scipy.optimize import dual_annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluation steps:\n",
    "1. Generate a long MD simulation (or an ensemble of shorter replica simulations)\n",
    "2. Apply diffusion maps and compute free energy $F(\\xi)$\n",
    "3. Divide by the diffusion coordinates into M boxes and calculate the transitions $C_{ij}$ over a specified time $t_{\\alpha}$. \n",
    "4. Compute the most probable rate matrix $\\mathbf{P}$ subject to a smoothness constraint on the diffusion coeffients. Compute the diffusion coefficients $D(\\xi)$.\n",
    "5. Identify the metastable states of interest, denoted $i$ and $j$, and compute the Kramer's rate of transition $\\nu\\left(i \\rightarrow j\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_name = 'alanine.pdb'\n",
    "save_name = 'trajectory.dcd'\n",
    "\n",
    "steps = 100\n",
    "iterations = 1000\n",
    "\n",
    "temperature = 300 \n",
    "R = 0.0083144621 # Universal Gas Constant kJ/K/mol\n",
    "beta = 1.0/(temperature*R) # units (kJ/mol)**(-1)\n",
    "\n",
    "friction_coefficient = 1.0/picosecond\n",
    "stepSize = 0.002*picoseconds\n",
    "\n",
    "def run_simulation(pdb_name, save_name, iterations, steps, temperature, force_fields=['amber14-all.xml', 'amber14/spce.xml']):\n",
    "    pdb = PDBFile(pdb_name)\n",
    "\n",
    "    forcefield = ForceField(*force_fields)\n",
    "    system = forcefield.createSystem(pdb.topology, nonbondedMethod=CutoffNonPeriodic, constraints=HBonds)\n",
    "\n",
    "    integrator = LangevinIntegrator(temperature*kelvin, friction_coefficient, stepSize)\n",
    "    simulation = Simulation(pdb.topology, system, integrator, platform=Platform.getPlatformByName('CUDA'))\n",
    "    simulation.context.setPositions(pdb.positions)\n",
    "    simulation.context.setVelocitiesToTemperature(temperature*kelvin)\n",
    "\n",
    "    mdinit = md.load_pdb(pdb_name)\n",
    "\n",
    "    mdinit.save_dcd(save_name)     \n",
    "    simulation.reporters.append(DCDReporter(save_name, steps, append=True))\n",
    "\n",
    "    max_count = iterations\n",
    "    bar = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    display(bar) # display the bar\n",
    "\n",
    "    for i in range(iterations):\n",
    "        bar.value += 1\n",
    "        simulation.step(steps)\n",
    "        state = simulation.context.getState(getEnergy=True, enforcePeriodicBox = False)\n",
    "        positions = simulation.context.getState(getPositions=True).getPositions()\n",
    "        \n",
    "    return simulation\n",
    "\n",
    "print('Simulation Time', 0.002*picoseconds*iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdinit = md.load_pdb(pdb_name)\n",
    "#simulation = run_simulation(pdb_name='alanine.pdb', save_name='trajectory.dcd', iterations=iterations, steps=steps, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_trajectory_file(traj_file, skip_first=1000, use_test_trajectory=False):\n",
    "    if use_test_trajectory:\n",
    "        traj_file = 'test_traj.dcd'\n",
    "    \n",
    "    traj_std_tmp = md.load_dcd(traj_file, mdinit.topology)\n",
    "    traj_orig = traj_std_tmp[skip_first:]\n",
    "    traj_orig = traj_orig.superpose(traj_orig[0])\n",
    "    \n",
    "    return traj_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = open_trajectory_file(traj_file='trajectory_long.dcd', skip_first=10, use_test_trajectory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply Diffusion Maps And Compute $F(\\xi)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply a target measure diffusion maps algorithm (TMDmap)\n",
    "\n",
    "weight_params = {}\n",
    "weight_params['simulation'] = simulation\n",
    "weight_params['temperature'] = temperature\n",
    "\n",
    "dmap_obj, traj_final = diffusionmap.compute_diffusionmaps(traj, nrpoints=None, epsilon=1.0, weights='compute', weight_params=weight_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump(dmap_obj, file = open(\"dmap_extra_long.pickle\", \"wb\"))\n",
    "dmap_obj = dill.load(open(\"dmap_extra_long.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdfeature.features as features\n",
    "psi = [6, 8, 14, 16]\n",
    "psi_torsion = features.compute_torsion_mdraj(traj, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(psi_torsion[30:40], label='psi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_coordinate = 0 \n",
    "\n",
    "def compute_marginalised_free_energy_from_diffusion_map(diffusion_map, diffusion_coordinate):\n",
    "    free_energy_counts, coordinate = np.histogram(diffusion_map.dmap[:,diffusion_coordinate], bins=200)\n",
    "    normalised_counts = free_energy_counts / np.sum(free_energy_counts)\n",
    "    with numpy.errstate(divide='ignore'):\n",
    "        free_energy = - (1/beta) * np.log(normalised_counts)\n",
    "        \n",
    "    for index, energy in enumerate(free_energy):\n",
    "        if energy == np.inf or energy == -np.inf:\n",
    "            free_energy[index] = np.nan\n",
    "        \n",
    "    return free_energy, coordinate[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and iterpolate free energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "free_energy, coordinate = compute_marginalised_free_energy_from_diffusion_map(dmap_obj, diffusion_coordinate)\n",
    "\n",
    "df = pd.DataFrame({'CV':coordinate, 'F':free_energy})\n",
    "si = MiceImputer(return_list=True, strategy={\"F\": \"interpolate\"},n=1)\n",
    "output = si.fit_transform(df)[0][1]\n",
    "\n",
    "plt.plot(coordinate, output.F, 'k')\n",
    "plt.xlabel('DC 1', fontsize=16)\n",
    "plt.ylabel('Free Energy (kJ/mol)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "diffusion_coordinate_time_series = dmap_obj.dmap[:,diffusion_coordinate]\n",
    "\n",
    "plt.hist(diffusion_coordinate_time_series, bins=20, color='k')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('DC1', fontsize=16)\n",
    "plt.ylabel('log counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition Coordinates and Compute Counts Matrix $C_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO: better ways of partitioning the state space involving using k-means; PyEmma</font>\n",
    "\n",
    "<font color='red'>TODO: make observational iternal on a $ps$ timescale</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(time_series, observation_interval, cells):\n",
    "    min_coord = min(time_series)\n",
    "    max_coord = max(time_series)\n",
    "    # the partitioning of the collective variable (here - uniform)\n",
    "    X = [min_coord + (n+0.5) * (max_coord-min_coord)/cells for n in range(cells)]\n",
    "    cell_sequence = []\n",
    "    for coord in time_series:\n",
    "        cell_sequence.append(round((cells-1)*(coord - min_coord)/(max_coord-min_coord)))\n",
    "\n",
    "    N = np.zeros((cells, cells))\n",
    "    state_sequence = [cell for idx, cell in enumerate(cell_sequence) if idx%observation_interval == 0]\n",
    "    for idx, cell in enumerate(state_sequence[:-1]):\n",
    "        N[cell, state_sequence[idx+1]]+=1\n",
    "    \n",
    "    P = np.zeros(cells)\n",
    "    for state in cell_sequence:\n",
    "        P[state] += 1\n",
    "        \n",
    "    P = P / np.sum(P)\n",
    "        \n",
    "    return N, P, X\n",
    "    \n",
    "t = 1    # the subsampling factor (\"observation interval\")\n",
    "    \n",
    "counts, P, Q = compute_counts(time_series=diffusion_coordinate_time_series, observation_interval=t, cells=20)\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.imshow(N)\n",
    "plt.title(r'$C_{ij}$', fontsize=16)\n",
    "plt.xlabel('j', fontsize=16)\n",
    "plt.ylabel('i', fontsize=16)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemma\n",
    "\n",
    "number_of_states = 8\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(diffusion_coordinate_time_series, k=10, stride=5, max_iter=150)\n",
    "discrete_traj = cluster.dtrajs[0]\n",
    "cluster_centers = cluster.clustercenters\n",
    "\n",
    "print(discrete_traj)\n",
    "\n",
    "def relabel_trajectory_by_cluster_center(discrete_traj, cluster_centers):\n",
    "    unique = np.unique(discrete_traj)\n",
    "    number_of_states = len(unique)\n",
    "    \n",
    "    Q = np.zeros(number_of_states)\n",
    "    for i in range(number_of_states):\n",
    "        Q[i] = cluster_centers[i]\n",
    "    \n",
    "    sorted_Q = np.sort(Q)\n",
    "    sorted_indices = np.argsort(np.argsort(Q))\n",
    "    \n",
    "    for idx, entry in enumerate(discrete_traj):\n",
    "        discrete_traj[idx] = sorted_indices[discrete_traj[idx]]\n",
    "        \n",
    "    \n",
    "    return discrete_traj, sorted_Q\n",
    "\n",
    "traj, Q = relabel_trajectory_by_cluster_center(discrete_traj, cluster_centers)\n",
    "\n",
    "print(traj)\n",
    "    \n",
    "# implied timescale analysis\n",
    "#its = pyemma.msm.its(cluster.dtrajs, lags=[1, 2, 3, 5, 7, 10], nits=3, errors='bayes')\n",
    "#pyemma.plots.plot_implied_timescales(its, ylog=False);\n",
    "\n",
    "# check for Markovian behaviour\n",
    "#bayesian_msm = pyemma.msm.bayesian_markov_model(cluster.dtrajs, lag=10, dt_traj='1 ps', conf=0.95)\n",
    "#pyemma.plots.plot_cktest(bayesian_msm.cktest(4), units='ps');\n",
    "\n",
    "def compute_C_matrix(discrete_trajectory, lag):\n",
    "    unique = np.unique(discrete_trajectory)\n",
    "    number_of_states = len(unique)\n",
    "    C = np.zeros((number_of_states, number_of_states))\n",
    "    for idx, state in enumerate(discrete_trajectory[:-lag]):\n",
    "        C[state, discrete_trajectory[idx+lag]]+=1\n",
    "        \n",
    "    return C\n",
    "    \n",
    "print(Q)\n",
    "counts = compute_C_matrix(traj, lag=1)\n",
    "plt.imshow(counts)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [-3, 2, -2, 0, 1, 0.5]\n",
    "np.argsort(np.argsort(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Most Probable $P_{ij}$ with Diffusion Coefficient Prior (Through Maximum Likelihood Optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "Commented out code - click here -\n",
    "<!---\n",
    "def convert_R_components_to_matrix(R_components, P, matrix_dimension):\n",
    "    R = np.zeros((matrix_dimension, matrix_dimension))\n",
    "    component_number = 0\n",
    "    for i in np.arange(0,matrix_dimension-1,1):\n",
    "        n = matrix_dimension - i\n",
    "        for j in range(i+1,i+n):\n",
    "            R[i][j] = R_components[component_number]\n",
    "            R[j][i] = R_components[component_number] * P[i] / P[j]\n",
    "            component_number += 1\n",
    "            \n",
    "    row_sums = np.sum(R, axis=1)\n",
    "    for i in range(matrix_dimension):\n",
    "        R[i][i] = - row_sums[i]\n",
    "        \n",
    "    return R\n",
    "                \n",
    "\n",
    "\n",
    "def negative_log_likelihood(R, *args):\n",
    "    # R is a 1D array\n",
    "    N = args[0]\n",
    "    P = args[1]\n",
    "    t = args[2]\n",
    "    number_of_cells = N.shape[0]\n",
    "\n",
    "    R = convert_R_components_to_matrix(R, P, number_of_cells)\n",
    "    P_half = P**(1/2)\n",
    "    P_minus_half = P**(-1/2)\n",
    "    \n",
    "    # column-wise multiplication by a vector\n",
    "    intermediate = np.multiply(R, P_half)\n",
    "    # row-wise multiplication by a vector\n",
    "    R_tilde = np.multiply(P_minus_half[:,np.newaxis], intermediate)\n",
    "    Lambda, U = np.linalg.eig(R_tilde)\n",
    "    #print(\"U\")\n",
    "    #print(U)\n",
    "    e_tLambda = np.diag(np.exp(t * Lambda))\n",
    "    #print(\"etLambda\")\n",
    "    #print(e_tLambda)\n",
    "    e_tR_tilde = U @ e_tLambda @ U.transpose()\n",
    "    #print(\"e_tR_tilde\")\n",
    "    #print(e_tR_tilde)\n",
    "    e_tR = np.diag(P**(1/2)) @ e_tR_tilde @ np.diag(P**(-1/2))\n",
    "    ln_e_tR = np.log(e_tR)\n",
    "    #print(ln_e_tR)\n",
    "    #input()\n",
    "    \n",
    "    nll = - np.sum(np.multiply(N, ln_e_tR))\n",
    "\n",
    "    return nll\n",
    "The rate matrix $\\mathbf{R}$ must satisfy detailed balance: for a given probability vector $P$, we have that $\\mathbf{R}$ has $N(N-1)/2$ free coefficients with matrix elements defined as:\n",
    "\n",
    "$$\n",
    "R_{ij} =\n",
    "\\begin{cases}\n",
    "R_{ij} > 0 &\\text{if}\\; i>j \\\\\n",
    "- \\sum_{l\\neq i}R_{li} &\\text{if}\\; i=j \\\\\n",
    "R_{ji}P_i/P_j &\\text{if}\\; i < j\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Now define the symmetric matrix $\\tilde{R}_{ij} = P^{-1/2}_i R_{ij} P_j^{1/2}$ whose matrix of real eigenvectors $U$ satisfies $\\mathbf{\\tilde{R}}\\mathbf{U} = \\mathbf{U}\\mathbf{\\Lambda}$, where $\\mathbf{\\Lambda}=\\text{diag}\\left(\\lambda_1, ..., \\lambda_N\\right)$. It follows that $e^{t\\mathbf{\\tilde{R}}}=\\mathbf{U}e^{t\\mathbf{\\Lambda}}\\mathbf{U}^T$ and so $e^{t\\mathbf{R}}=\\text{diag}\\left(P_1^{1/2}, ..., P_N^{1/2}\\right)e^{t\\mathbf{\\tilde{R}}}\\text{diag}\\left(P_1^{-1/2},...,P_N^{-1/2}\\right)$ which can be calculated by diagonalisation of the real symmetric matrix $\\mathbf{\\tilde{R}}$.\n",
    "\n",
    "The log-likelihood function associated with the observed $N_{ij}$ for a given rate matrix $\\mathbf{R}$ is then:\n",
    "\n",
    "$$\n",
    "\\ln{L} = \\sum_{i=1}^{m}\\sum_{j=1}^m N_{ij} \\ln{\\left(e^{tR}\\right)_{ij}}.\n",
    "$$\n",
    "<font color='red'>TODO: Compute the stationary distribution from the $P_i$; PyEmma</font>\n",
    "\n",
    "It can be shown that the convex constrained optimisation problem:\n",
    "\n",
    "$$\n",
    "\\text{minimise} - \\sum_{i,j} c_{ij} \\log{p_{ij}}\n",
    "$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\n",
    "p_{ij} \\geq 0 \\\\\n",
    "\\sum_{j} p_{ij} = 1 \\\\\n",
    "\\pi_i p_{ij} = \\pi_j p_{ji}\n",
    "$$\n",
    "\n",
    "can be solved by fixed-point iteration with solution\n",
    "\n",
    "$$\n",
    "p^*_{ij} = \\frac{\\left( c_{ij} + c_{ji}\\right)\\pi_i}{\\lambda_i^*\\pi_j + \\lambda_j^*\\pi_i}\n",
    "$$\n",
    "\n",
    "where the Lagrangian parameters $\\lambda_i^*$ are obtained by iterating the following self-consistent equation to convergence\n",
    "\n",
    "$$\n",
    "\\lambda_i^{(n+1)} = \\sum_j \\frac{\\left(c_{ij}+c_{ji}\\right) \\pi_j \\lambda_i^{(n)}}{\\lambda_i^{(n)}\\pi_j + \\lambda_{j}^{(n)}\\pi_i}.\n",
    "$$\n",
    "def update_lambda(old_lambda, counts, stationary_distribution):\n",
    "    intermediate_matrix = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            intermediate_matrix[i][j] = (counts[i][j] + counts[j][i])*(stationary_distribution[j]*old_lambda[i])/(old_lambda[i]*stationary_distribution[j] + old_lambda[j]*stationary_distribution[i])\n",
    "            \n",
    "    return np.sum(intermediate_matrix, axis=0)\n",
    "\n",
    "def calculate_lagrangian_parameters(counts, stationary_distribution, err):\n",
    "    lambda_vec = np.random.normal(size=counts.shape[0])\n",
    "    current_error = float('inf')\n",
    "    iterations = 0\n",
    "    while current_error > err:\n",
    "        new_lambda_vec = update_lambda(lambda_vec, counts, stationary_distribution)\n",
    "        current_error = np.linalg.norm(new_lambda_vec-lambda_vec)\n",
    "        lambda_vec = new_lambda_vec\n",
    "        iterations += 1\n",
    "        if iterations % 5 == 0:\n",
    "            print(iterations)\n",
    "    \n",
    "    return lambda_vec\n",
    "    \n",
    "def calculate_rate_matrix(counts, stationary_distribution, err):\n",
    "    lambda_vec = calculate_lagrangian_parameters(counts, stationary_distribution, err)\n",
    "    \n",
    "    rate_matrix = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            rate_matrix[i][j] = (counts[i][j] + counts[j][i])*(stationary_distribution[i])/(lambda_vec[i]*stationary_distribution[j] + lambda_vec[j]*stationary_distribution[i])\n",
    "            \n",
    "    return rate_matrix\n",
    "\n",
    "rate_matrix = calculate_rate_matrix(counts=N, stationary_distribution=P, err = 0.001)\n",
    "\n",
    "stationary_distribution, rate_matrix_gamma = fpi.fit_markov_state_model(counts=N, coordinates=coordinate, gamma=0.0001, error=0.0001)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.imshow(rate_matrix)\n",
    "plt.title(r'$R_{ij}$', fontsize=16)\n",
    "plt.xlabel('i', fontsize=16)\n",
    "plt.ylabel('j', fontsize=16)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "plt.imshow(rate_matrix_gamma.T)\n",
    "plt.title(r'$R_{ij}$', fontsize=16)\n",
    "plt.xlabel('i', fontsize=16)\n",
    "plt.ylabel('j', fontsize=16)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "def update_lambda(old_lambda, counts, stationary_distribution):\n",
    "    intermediate_matrix = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            intermediate_matrix[i][j] = (counts[i][j] + counts[j][i])*(stationary_distribution[j]*old_lambda[i])/(old_lambda[i]*stationary_distribution[j] + old_lambda[j]*stationary_distribution[i])\n",
    "            \n",
    "    return np.sum(intermediate_matrix, axis=0)\n",
    "\n",
    "def calculate_lagrangian_parameters(counts, stationary_distribution, err):\n",
    "    lambda_vec = np.random.normal(size=counts.shape[0])\n",
    "    current_error = float('inf')\n",
    "    iterations = 0\n",
    "    while current_error > err:\n",
    "        new_lambda_vec = update_lambda(lambda_vec, counts, stationary_distribution)\n",
    "        current_error = np.linalg.norm(new_lambda_vec-lambda_vec)\n",
    "        lambda_vec = new_lambda_vec\n",
    "        iterations += 1\n",
    "        if iterations % 5 == 0:\n",
    "            print(iterations)\n",
    "    \n",
    "    return lambda_vec\n",
    "    \n",
    "def calculate_rate_matrix(counts, stationary_distribution, err):\n",
    "    lambda_vec = calculate_lagrangian_parameters(counts, stationary_distribution, err)\n",
    "    \n",
    "    rate_matrix = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            rate_matrix[i][j] = (counts[i][j] + counts[j][i])*(stationary_distribution[i])/(lambda_vec[i]*stationary_distribution[j] + lambda_vec[j]*stationary_distribution[i])\n",
    "            \n",
    "    return rate_matrix\n",
    "\n",
    "P = calculate_rate_matrix(counts=counts, stationary_distribution=P, err = 0.001)\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for inforcing symmetry contraints \n",
    "\n",
    "def random_initialise_symmetric_components(matrix_shape):\n",
    "    # e.g. if matrix shape is (3,3) then produces a random vector of 6 components\n",
    "    dim = int(matrix_shape[0] * (matrix_shape[0] + 1) / 2)\n",
    "    x0 = np.random.uniform(size=dim)\n",
    "    \n",
    "    return x0\n",
    "\n",
    "def get_symmetric_components_of_matrix(matrix):\n",
    "    dim = matrix.shape[0]\n",
    "    symmetric_components = []\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if j >= i:\n",
    "                symmetric_components.append(matrix[i][j])\n",
    "                \n",
    "    return np.array(symmetric_components)\n",
    "\n",
    "\n",
    "def construct_matrix_from_symmetric_components(symmetric_components):\n",
    "    dim = int((-1+np.sqrt(1+8*symmetric_components.size))/2)\n",
    "    matrix = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if j >= i:\n",
    "                matrix[i][j] = symmetric_components[int(i*dim - i*(i-1)/2 + j - i)]\n",
    "                matrix[j][i] = matrix[i][j]  # symmetrise\n",
    "                \n",
    "    return matrix\n",
    "\n",
    "\n",
    "# utility function for computing diffusion coefficient domain \n",
    "def domain_of_diffusion_coefficient(Q):\n",
    "    Q_diff = []\n",
    "    for i in range(len(Q)-1):\n",
    "        Q_diff.append((Q[i+1]+Q[i])/2)\n",
    "        \n",
    "    return Q_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint\n",
    "\n",
    "def compute_diffusion_coefficients(Q, X, x):\n",
    "    D = np.zeros(len(Q) - 1)\n",
    "    deltaQ2s = []\n",
    "    for i in range(len(Q) - 1):\n",
    "        deltaQ2 = (Q[i + 1] - Q[i]) ** 2\n",
    "        D[i] = deltaQ2 * (X[i][i + 1]*X[i+1][i] / x[i]*x[i+1]) ** (0.5)\n",
    "        deltaQ2s.append(deltaQ2)\n",
    "\n",
    "    return D, np.mean(deltaQ2s)\n",
    "\n",
    "\n",
    "def compute_diff_coeff_smoothness_penalty(D, gamma, Qvar):\n",
    "    smoothness_penalty = 0\n",
    "    for i in range(len(D) - 1):\n",
    "        smoothness_penalty += (D[i] - D[i + 1]) ** 2 / (2 * (Qvar * gamma) ** 2)\n",
    "\n",
    "    return smoothness_penalty\n",
    "\n",
    "\n",
    "def negative_log_likelihood(X, counts, coordinates, gamma, use_smoothing, barrier_penalty=0.001):\n",
    "    X = construct_matrix_from_symmetric_components(symmetric_components = X)\n",
    "    x = np.sum(X, axis=1) #col sum\n",
    "\n",
    "    if use_smoothing:\n",
    "        D, Qvar = compute_diffusion_coefficients(Q, X, x)\n",
    "        smoothness_penalty = compute_diff_coeff_smoothness_penalty(D, gamma, Qvar)\n",
    "    else:\n",
    "        smoothness_penalty = 0\n",
    "        \n",
    "    A = X/x\n",
    "    \n",
    "    barrier_function = barrier_penalty*(np.sum(- np.log(x))+np.sum(- np.log(1-x)))\n",
    "    \n",
    "    nll = float(- np.sum(counts * np.log(A)) + smoothness_penalty + barrier_function)\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_smoothing=True\n",
    "\n",
    "number_of_parameters = int(counts.shape[0]*(counts.shape[0]+1)/2)\n",
    "A = np.eye(number_of_parameters)\n",
    "b = np.zeros(number_of_parameters)\n",
    "lincon = LinearConstraint(A, b, np.inf*np.ones(number_of_parameters), keep_feasible=True)\n",
    "\n",
    "# start from a position slightly perturbed from the optimal without a smoothness penalty\n",
    "X0 = np.random.uniform(0., 0.05, size=(counts.shape))+((counts + counts.T)/(2*np.sum(counts)))\n",
    "x0 = get_symmetric_components_of_matrix(X0)\n",
    "\n",
    "#for gamma in [0.0000001, 0.00000005, 0.00000001, 0.000000005, 0.000000001]:\n",
    "res = minimize(method='Nelder-Mead', fun=negative_log_likelihood, x0 = x0, args=(counts, Q, 0.01, use_smoothing), constraints=(lincon,), options={'maxiter': 20000})\n",
    "X_star = construct_matrix_from_symmetric_components(res.x)\n",
    "x_star = np.sum(X_star, axis=1)\n",
    "pi_star = x_star/np.sum(x_star)\n",
    "P_star = X_star/x_star\n",
    "plt.imshow(P_star)\n",
    "plt.colorbar()\n",
    "D, Qvar = compute_diffusion_coefficients(Q, X_star, x_star)\n",
    "plt.show()\n",
    "plt.plot(domain_of_diffusion_coefficient(Q), D)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diffusion_coefficients_from_transition_matrix(Q, P):\n",
    "    D = np.zeros(len(Q) - 1)\n",
    "    deltaQ2s = []\n",
    "    for i in range(len(Q) - 1):\n",
    "        deltaQ2 = (Q[i + 1] - Q[i]) ** 2\n",
    "        D[i] = deltaQ2 * (P[i][i + 1]*P[i+1][i]) ** (0.5)\n",
    "        deltaQ2s.append(deltaQ2)\n",
    "\n",
    "    return D, np.mean(deltaQ2s)\n",
    "\n",
    "stationary_distribution, P = fpi.fit_markov_state_model(counts=counts, coordinates=coordinate, gamma=100, error=0.0001)\n",
    "\n",
    "D, _ = compute_diffusion_coefficients_from_transition_matrix(Q, P)\n",
    "plt.imshow(P)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.plot(domain_of_diffusion_coefficient(Q), D)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import scipy.interpolate as interpolate\n",
    "\n",
    "\n",
    "\n",
    "F_smoothed = savgol_filter(output.F, 15, 3)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(x_range, D_smoothed, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(coordinate, F_smoothed, color='k')\n",
    "\n",
    "plt.legend([l1, l2], [\"diffusion_coefficient\", \"free_energy\"])\n",
    "\n",
    "well_minima = find_peaks(-F_smoothed, prominence=2)\n",
    "for idx, minima in enumerate(well_minima[0]):\n",
    "    print(\"Minima \", (round(coordinate[minima],3), round(F_smoothed[minima],3)))\n",
    "    plt.vlines(coordinate[minima], ymin = 0, ymax = max(F_smoothed))\n",
    "    plt.text( coordinate[minima]+0.02*(max(coordinate)-min(coordinate)), F_smoothed[minima], f\"S{idx}\", fontsize=16, color='b')\n",
    "\n",
    "ax1.set_xlabel('DC 1', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "D_smoothed = D #savgol_filter(D, 5, 3)\n",
    "F_smoothed = savgol_filter(free_energy, 15, 3)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(Xd, D_smoothed, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(coordinate, F_smoothed, color='k')\n",
    "\n",
    "plt.legend([l1, l2], [\"diffusion_coefficient\", \"free_energy\"])\n",
    "\n",
    "well_minima = find_peaks(-F_smoothed, prominence=1.5)\n",
    "for idx, minima in enumerate(well_minima[0]):\n",
    "    print(\"Minima \", (round(coordinate[minima],3), round(F_smoothed[minima],3)))\n",
    "    plt.vlines(coordinate[minima], ymin = -10, ymax = 0)\n",
    "    plt.text( coordinate[minima]+0.02*(max(coordinate)-min(coordinate)), F_smoothed[minima], f\"S{idx}\", fontsize=16, color='b')\n",
    "\n",
    "ax1.set_xlabel('DC 1', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute the Kramer's Rate Along the Collective Variable Transection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>TODO: Figure out why this isn't invariant to adding arbitrary constant to $F(x)$</font>\n",
    "\n",
    "Example: Calculating Kramer's rate for trasition $S1 \\rightarrow S2$\n",
    "\n",
    "Kramer's rate $\\nu(i\\rightarrow j)$ is given by\n",
    "\n",
    "$$\n",
    "\\nu(i \\rightarrow j) = \\left( \\int_{\\mathcal{B_{ij}}} \\frac{e^{\\beta F(x)}}{D(x)} dx \\int_{\\mathcal{W}_{ij}} e^{- \\beta F(x')} dx' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "where the barrier region $\\mathcal{B}_{ij}$ is the segment of configurational space between (minima) states $i$ and $j$ and $\\mathcal{W}_{ij}$ is the half of $\\mathcal{B}_{ij}$ that is closer to state $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "\n",
    "initial_state = 0\n",
    "final_state = 1\n",
    "\n",
    "ix = well_minima[0][initial_state]\n",
    "jx = well_minima[0][final_state]\n",
    "print(ix, jx)\n",
    "midx = int(np.floor((ix+jx)/2))\n",
    "\n",
    "def compute_well_integrand(beta, F):\n",
    "    integrand = np.zeros(len(F))\n",
    "    for x in range(len(F)):\n",
    "        integrand[x] = np.exp(- beta * F[x])\n",
    "    \n",
    "    return integrand\n",
    "\n",
    "def interpolate_function(x_coords, y_coords, x_to_evaluate):  \n",
    "    x_min = min(x_coords)\n",
    "    x_max = max(x_coords)\n",
    "    \n",
    "    if min(x_coords) <= x_to_evaluate <= max(x_coords):\n",
    "        # interpolate\n",
    "        x_low = max([x for x in x_coords if x <= x_to_evaluate])\n",
    "        i_low = np.where(x_coords == x_low)\n",
    "        x_high = min([x for x in x_coords if x > x_to_evaluate])\n",
    "        i_high = np.where(x_coords == x_high)\n",
    "        interpolation_distance = (x_to_evaluate - x_low)/(x_high - x_low)\n",
    "\n",
    "        return y_coords[i_low] + (y_coords[i_high] - y_coords[i_low])*interpolation_distance \n",
    "    \n",
    "    elif x_to_evaluate < x_min:\n",
    "        # extrapolate\n",
    "        return y_coords[0] - (x_min - x_to_evaluate)*(y_coords[1] - y_coords[0])/(x_coords[1] - x_coords[0])\n",
    "        \n",
    "    elif x_to_evaluate > x_max:\n",
    "        # extrapolate\n",
    "        return y_coords[-1] + (x_to_evaluate - x_max)*(y_coords[-1] - y_coords[-2])/(x_coords[-1] - x_coords[-2])\n",
    "\n",
    "def compute_barrier_integrand(beta, F, D, coordinate):\n",
    "    integrand = np.zeros(len(F))\n",
    "    for x in range(len(F)):\n",
    "        integrand[x] = np.exp(beta * F[x])/interpolate_function(x_range, D, coordinate[x])\n",
    "    \n",
    "    return integrand\n",
    "\n",
    "well_integrand = compute_well_integrand(beta, F_smoothed)\n",
    "    \n",
    "well_integral = integrate.simps(well_integrand[ix:midx+1], coordinate[ix:midx+1])\n",
    "print('Well integral: ', well_integral)\n",
    "\n",
    "\n",
    "barrier_integrand = compute_barrier_integrand(beta, F_smoothed, D_smoothed, coordinate)\n",
    "    \n",
    "barrier_integral = integrate.simps(barrier_integrand[ix+1:jx], coordinate[ix+1:jx])\n",
    "print('Barrier integral: ', barrier_integral)\n",
    "\n",
    "kramers_rate = (barrier_integral * well_integral)**(-1)\n",
    "print(\"Kramer's Rate: \", kramers_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 300 \n",
    "beta = 1.0/(temperature*0.0083144621) # units kJ/mol\n",
    "\n",
    "#dmap_obj = dill.load(open(\"dmap_long.pickle\", \"rb\"))\n",
    "diffusion_coordinate = 0\n",
    "diffusion_coordinate_trajectory = dmap_obj.dmap[:,diffusion_coordinate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kre = KramersRateEvaluator(verbose=True)\n",
    "\n",
    "ps = 10**(-12)\n",
    "rates = kre.fit(diffusion_coordinate_trajectory, \n",
    "                beta, \n",
    "                sigmaD=0.002,\n",
    "                sigmaF=0.00025,\n",
    "                lag = 12,\n",
    "                bins=200, \n",
    "                step_size=0.002*ps,\n",
    "                minima_prominance=2.0, \n",
    "                cluster_type='kmeans',\n",
    "                options={'k': 50, 'stride': 5, 'max_iter': 150,\n",
    "                 'max_centers': 1000, 'metric': 'euclidean', 'n_jobs': None, 'dmin': 0.002})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_rate(start_state, end_state, rates):\n",
    "    return [rate for rate in rates if rate[0][0] == start_state and rate[0][1] == end_state][0][1]\n",
    "\n",
    "ks = [10,25,50,75,100,200]\n",
    "dmins = [0.005, 0.0025, 0.001, 0.0005, 0.00025, 0.0001]\n",
    "\n",
    "kre = KramersRateEvaluator()\n",
    "\n",
    "rates_data = []\n",
    "\n",
    "kre.fit(diffusion_coordinate_trajectory, \n",
    "        beta=1, \n",
    "        sigmaD=0.075,\n",
    "        sigmaF=0.01,\n",
    "        step_size=4e-2,\n",
    "        lag=5,\n",
    "        bins=300, \n",
    "        impute_free_energy_nans = False,\n",
    "        minima_prominance=1.1, \n",
    "        include_endpoint_minima=False,\n",
    "        cluster_type='kmeans',\n",
    "        options={'k': 50, 'stride': 5, 'max_iter': 150,\n",
    "         'max_centers': 20, 'metric': 'euclidean', 'n_jobs': None, 'dmin': 0.002})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rates_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
